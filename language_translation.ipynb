{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Checking if GPU is running or not\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KALYvZycPe8N",
        "outputId": "acee4408-1bc0-4927-9656-ba04d6c8361e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Nov 26 14:02:50 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0    27W /  70W |   4601MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers[sentencepiece] sacrebleu -q"
      ],
      "metadata": {
        "id": "_3XQ_jhwe0m5"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "fr_hhoAFefza"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import transformers\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "from transformers import AdamWeightDecay\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "-dbDT2O9fJEp"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"Helsinki-NLP/opus-mt-mul-en\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q8AHfSWWMLm"
      },
      "source": [
        "## Helsinki-NLP/opus-mt-mul-en\n",
        "\n",
        "source: https://huggingface.co/Helsinki-NLP/opus-mt-mul-en\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H3frhJQ_Lym"
      },
      "source": [
        "# The Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFmXYJQHBetF"
      },
      "source": [
        "Dataset: snow_simplified_japanese_corpus\n",
        "Source: https://huggingface.co/datasets/snow_simplified_japanese_corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "daj9JBV6jDGK"
      },
      "outputs": [],
      "source": [
        "raw_datasets = load_dataset(\"snow_simplified_japanese_corpus\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHdy5o8zsGhm",
        "outputId": "27add4b1-e844-457e-f7e8-61e67cd664a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['ID', 'original_ja', 'simplified_ja', 'original_en'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.9\n",
        "val_ratio = 0.05\n",
        "test_ratio = 0.05"
      ],
      "metadata": {
        "id": "zijE3wOVhwpA"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = len(raw_datasets[\"train\"])\n",
        "num_train = int(num_samples * train_ratio)\n",
        "num_val = int(num_samples * val_ratio)\n",
        "num_test = num_samples - num_train - num_val"
      ],
      "metadata": {
        "id": "IGoXMGORips8"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = {\n",
        "    \"train\": raw_datasets[\"train\"].select(list(range(num_train))),\n",
        "    \"validation\": raw_datasets[\"train\"].select(list(range(num_train, num_train + num_val))),\n",
        "    \"test\": raw_datasets[\"train\"].select(list(range(num_train + num_val, num_samples))),\n",
        "}\n"
      ],
      "metadata": {
        "id": "rkT8vnmNphmY"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "split_dataset = DatasetDict(splits)"
      ],
      "metadata": {
        "id": "IwXq_Kfcposd"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ff_cUAVpzyh",
        "outputId": "897a70bd-0625-4f7a-845d-ae21961e054e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['ID', 'original_ja', 'simplified_ja', 'original_en'],\n",
              "        num_rows: 45000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['ID', 'original_ja', 'simplified_ja', 'original_en'],\n",
              "        num_rows: 2500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['ID', 'original_ja', 'simplified_ja', 'original_en'],\n",
              "        num_rows: 2500\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfaNkUHNqU11",
        "outputId": "9979e0e5-e60b-43b9-c1bc-9397bc03b917"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ID': '2',\n",
              " 'original_ja': '多くの動物が人間によって滅ぼされた。',\n",
              " 'simplified_ja': '多くの動物が人間によって殺された。',\n",
              " 'original_en': 'many animals have been destroyed by men .'}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "split_dataset['train'][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x6TtxnEEDuw"
      },
      "source": [
        "#Preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "20qWg7z8qv7z"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYCILyEK0HXG",
        "outputId": "89fadcaa-e958-42d6-feab-29897a75b2ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [17115, 146, 3, 73, 17, 9, 4, 5210, 4084, 58, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "tokenizer(\"Hello, this is a sentence!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDgcI2Nz0SZN",
        "outputId": "ffb65223-37c6-413f-a164-f9ea728d1c81"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[17115, 146, 3, 73, 17, 9, 4, 5210, 4084, 58, 0], [6583, 269, 17, 1905, 6205, 4, 5210, 4084, 2, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "tokenizer([\"Hello, this is a sentence!\", \"This is another sentence.\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh0epnye1JJk",
        "outputId": "261d0cb1-3375-4a8a-8372-e0dd4f1f5dbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[4, 15541, 2504, 8231, 9389, 3643, 26994, 14214, 29846, 7317, 14387, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ],
      "source": [
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer([\"私の名前はチラグです\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "XHaFoHKG3snJ"
      },
      "outputs": [],
      "source": [
        "max_input_length = 128\n",
        "max_target_length = 128\n",
        "\n",
        "source_lang = \"original_en\"\n",
        "target_lang = \"original_ja\"\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[source_lang]\n",
        "    targets = examples[target_lang]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX9vN5Yo4kuG",
        "outputId": "776ff803-060d-4dad-9924-a3de7dfaf5fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[32, 96, 341, 28, 37, 325, 557, 338, 1442, 325, 365, 182, 221, 5188, 256, 4, 2, 0], [95, 288, 6669, 6, 121, 221, 31, 128, 43, 3907, 237, 44, 70, 334, 4, 2, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[4, 30593, 5116, 2187, 41481, 3605, 15660, 7547, 4703, 15541, 3605, 3643, 5239, 4703, 4538, 8012, 20566, 7307, 445, 0], [4, 5706, 7547, 2504, 17891, 10557, 5116, 1858, 17443, 3605, 10905, 7720, 8983, 1, 12166, 5699, 5853, 445, 0]]}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "preprocess_function(split_dataset[\"train\"][:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "PTzg3eWK4_FG"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = split_dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "LEbE41CbU1bL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8122cc-8f87-4d7c-b73e-738bceb57dfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
            "\n",
            "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-mul-en.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "GOJU15XzX5iy"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01\n",
        "num_train_epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "NCoU8kNtX6Tk"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "4IbSTkjuYZ8Z"
      },
      "outputs": [],
      "source": [
        "generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "AgvwEEvDZP7F"
      },
      "outputs": [],
      "source": [
        "train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets[\"test\"],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "E_1LNLwvZQvD"
      },
      "outputs": [],
      "source": [
        "validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets[\"validation\"],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "GYF9ApeHZTR5"
      },
      "outputs": [],
      "source": [
        "generation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets[\"validation\"],\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    collate_fn=generation_data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Wx6p1NQ_ZVXb"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
        "model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "-G5WXVp0Z15g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b823e1b5-3d6b-450a-fcb1-cca589a8e7ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "156/156 [==============================] - 101s 417ms/step - loss: 5.6878 - val_loss: 3.9503\n",
            "Epoch 2/20\n",
            "156/156 [==============================] - 37s 240ms/step - loss: 3.8367 - val_loss: 3.5626\n",
            "Epoch 3/20\n",
            "156/156 [==============================] - 37s 239ms/step - loss: 3.5273 - val_loss: 3.3059\n",
            "Epoch 4/20\n",
            "156/156 [==============================] - 47s 302ms/step - loss: 3.2725 - val_loss: 3.1079\n",
            "Epoch 5/20\n",
            "156/156 [==============================] - 47s 303ms/step - loss: 3.0592 - val_loss: 2.9287\n",
            "Epoch 6/20\n",
            "156/156 [==============================] - 47s 303ms/step - loss: 2.8796 - val_loss: 2.8138\n",
            "Epoch 7/20\n",
            "156/156 [==============================] - 36s 232ms/step - loss: 2.7223 - val_loss: 2.6998\n",
            "Epoch 8/20\n",
            "156/156 [==============================] - 37s 238ms/step - loss: 2.5710 - val_loss: 2.6097\n",
            "Epoch 9/20\n",
            "156/156 [==============================] - 37s 237ms/step - loss: 2.4280 - val_loss: 2.5161\n",
            "Epoch 10/20\n",
            "156/156 [==============================] - 37s 238ms/step - loss: 2.2935 - val_loss: 2.4576\n",
            "Epoch 11/20\n",
            "156/156 [==============================] - 37s 237ms/step - loss: 2.1562 - val_loss: 2.3817\n",
            "Epoch 12/20\n",
            "156/156 [==============================] - 37s 238ms/step - loss: 2.0370 - val_loss: 2.3201\n",
            "Epoch 13/20\n",
            "156/156 [==============================] - 37s 237ms/step - loss: 1.9176 - val_loss: 2.2799\n",
            "Epoch 14/20\n",
            "156/156 [==============================] - 37s 237ms/step - loss: 1.8183 - val_loss: 2.2421\n",
            "Epoch 15/20\n",
            "156/156 [==============================] - 37s 237ms/step - loss: 1.7161 - val_loss: 2.2260\n",
            "Epoch 16/20\n",
            "156/156 [==============================] - 47s 301ms/step - loss: 1.6216 - val_loss: 2.1674\n",
            "Epoch 17/20\n",
            "156/156 [==============================] - 47s 304ms/step - loss: 1.5253 - val_loss: 2.1707\n",
            "Epoch 18/20\n",
            "156/156 [==============================] - 37s 240ms/step - loss: 1.4391 - val_loss: 2.1394\n",
            "Epoch 19/20\n",
            "156/156 [==============================] - 47s 303ms/step - loss: 1.3663 - val_loss: 2.1374\n",
            "Epoch 20/20\n",
            "156/156 [==============================] - 37s 239ms/step - loss: 1.2846 - val_loss: 2.1283\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e8e9f802110>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "model.fit(train_dataset, validation_data=validation_dataset, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "4EWkhFnyZ3Z8"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"tf_model/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcGNz4yrMY4F"
      },
      "source": [
        "# Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "TTkqZ2fuLbd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57e3998d-159c-45ce-8219-0eb5bd4821df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
            "\n",
            "All the layers of TFMarianMTModel were initialized from the model checkpoint at tf_model/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"tf_model/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "C4-NztUuM6pk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "599b5dd6-3e8e-4f7b-f462-30e1437d8fb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[64171     4 15541  2504  8231  9389  3643   391 14214 15127  7317 14387\n",
            "    445     0]], shape=(1, 14), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "input_text  = \"My name is chirag.\"\n",
        "\n",
        "tokenized = tokenizer([input_text], return_tensors='np')\n",
        "out = model.generate(**tokenized, max_length=128)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pfilqii_DwZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "2OIrGT1uOjKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7765f24-b5de-43fe-cefd-cfabdb77f17c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私の名前は chiラクです。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
        "\n",
        "\n",
        "decoded_translations = tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n",
        "\n",
        "# Example reference translations in Japanese\n",
        "reference_translations = [\n",
        "    \"私の名前はチラグです\"\n",
        "]\n",
        "\n",
        "# Calculate BLEU score\n",
        "overall_bleu_score = corpus_bleu([reference_translations], [decoded_translations], smoothing_function=None)\n",
        "\n",
        "# Token-wise BLEU scores\n",
        "token_bleu_scores = []\n",
        "for n in range(1, 5):  # You can adjust the n-gram range\n",
        "    token_bleu = sentence_bleu([reference_translations[0].split()], decoded_translations.split(), weights=(1/n,)*n)\n",
        "    token_bleu_scores.append(token_bleu)\n",
        "\n",
        "print(f\"Overall BLEU Score: {overall_bleu_score * 100:.2f}\")\n",
        "#print(f\"Token-wise BLEU Scores: {token_bleu_scores}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_WjVcYschmbn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbb25036-d869-437c-91c2-333947f7c4a4"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall BLEU Score: 31.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Y1EReq_DIcP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}